# Distributed Inference System â€” Default Configuration

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  dtype: "float16"
  num_layers: 22
  hidden_size: 2048
  intermediate_size: 5632
  num_attention_heads: 32
  num_kv_heads: 4
  max_sequence_length: 2048

coordinator:
  host: "localhost"
  port: 50050
  heartbeat_interval_sec: 5.0
  heartbeat_timeout_sec: 15.0
  failure_threshold: 3
  max_concurrent_requests: 4
  min_vram_mb: 512
  min_compute_tflops: 0.5
  gpu_required: false
  rebalance_cooldown_sec: 5.0
  rebalance_drain_timeout_sec: 30.0
  allocation_alpha_latency: 0.7
  allocation_beta_throughput: 0.3
  default_bandwidth_mbps: 1000.0
  default_latency_ms: 5.0
  memory_safety_margin: 0.9

node:
  host: "localhost"
  device: "auto"
  coordinator_address: "localhost:50050"
  max_cached_requests: 1
  max_cache_tokens_per_request: 4096
  cache_eviction_policy: "lru"

inference:
  max_tokens: 50
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  enable_kv_cache: true

communication:
  compress_tensors: false
  max_message_size_mb: 256
