# Distributed Inference System â€” Default Configuration

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  dtype: "float16"
  num_layers: 22
  hidden_size: 2048
  intermediate_size: 5632
  num_attention_heads: 32
  num_kv_heads: 4
  max_sequence_length: 2048

coordinator:
  host: "localhost"
  port: 50050
  heartbeat_interval_sec: 5.0
  heartbeat_timeout_sec: 15.0
  failure_threshold: 3
  max_concurrent_requests: 4
  max_concurrent_requests_global: 4
  max_queue_size: 32
  enable_concurrent_scheduler: false
  scheduler_policy: "balanced"
  fairness_quantum_tokens: 16
  tail_latency_guardrail_ms: 2500.0
  per_node_vram_safety_margin: 0.9
  scheduler_tick_ms: 10
  max_dispatch_per_tick: 8
  max_retry_attempts: 2
  retry_backoff_ms: 25
  ready_wait_timeout_sec: 30.0
  ready_poll_interval_ms: 100
  min_vram_mb: 512
  min_compute_tflops: 0.5
  gpu_required: false
  rebalance_cooldown_sec: 5.0
  node_load_failure_backoff_sec: 30.0
  rebalance_drain_timeout_sec: 30.0
  allocation_alpha_latency: 0.7
  allocation_beta_throughput: 0.3
  default_bandwidth_mbps: 1000.0
  default_latency_ms: 5.0
  memory_safety_margin: 0.9

node:
  host: "localhost"
  device: "auto"
  coordinator_address: "localhost:50050"
  max_cached_requests: 8
  max_concurrent_lanes: 4
  warm_context_pool_size: 2
  max_prefill_batch_size: 1
  max_cache_tokens_per_request: 4096
  cache_eviction_policy: "lru"

inference:
  max_tokens: 50
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  enable_kv_cache: true

communication:
  compress_tensors: false
  max_message_size_mb: 256
